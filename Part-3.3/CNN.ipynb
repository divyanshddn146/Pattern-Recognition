{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import math, time\n",
        "from tensorflow.keras import layers, models, losses, optimizers\n",
        "\n",
        "# Enable mixed precision (faster on GPUs like V100/A100)\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# ------------------ ResNet18 building blocks ------------------\n",
        "class BasicBlock(tf.keras.Model):\n",
        "    expansion = 1\n",
        "    def __init__(self, filters, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = layers.Conv2D(filters, 3, strides=stride, padding=\"same\", use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.relu = layers.ReLU()\n",
        "        self.conv2 = layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        # Shortcut (skip connection):\n",
        "        # If stride=1 → input and output shapes match → identity mapping\n",
        "        # If stride=2 → use 1x1 conv to downsample input to correct shape\n",
        "        if stride != 1:\n",
        "            self.shortcut = models.Sequential([\n",
        "                layers.Conv2D(filters, 1, strides=stride, use_bias=False),\n",
        "                layers.BatchNormalization()\n",
        "            ])\n",
        "        else:\n",
        "            self.shortcut = lambda x: x\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        out = self.relu(self.bn1(self.conv1(x), training=training))\n",
        "        out = self.bn2(self.conv2(out), training=training)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "def make_layer(filters, blocks, stride=1):\n",
        "    layers_ = [BasicBlock(filters, stride)]\n",
        "    for _ in range(1, blocks):\n",
        "        layers_.append(BasicBlock(filters, 1))\n",
        "    return models.Sequential(layers_)\n",
        "\n",
        "\n",
        " # Input: CIFAR-10 images (32,32,3).\n",
        "\n",
        "    # Stem: Conv → BN → ReLU.\n",
        "\n",
        "    # 4 stages: 64, 128, 256, 512 filters.\n",
        "\n",
        "    # Downsamples via stride=2 at each stage.\n",
        "\n",
        "    # Global average pool → Dense layer → logits for 10 classes\n",
        "def ResNet18(num_classes=10):\n",
        "    inputs = layers.Input(shape=(32,32,3))\n",
        "    x = layers.Conv2D(64, 3, strides=1, padding=\"same\", use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = make_layer(64, 2, stride=1)(x)\n",
        "    x = make_layer(128, 2, stride=2)(x)\n",
        "    x = make_layer(256, 2, stride=2)(x)\n",
        "    x = make_layer(512, 2, stride=2)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes)(x)\n",
        "\n",
        "    return models.Model(inputs, x)\n",
        "\n",
        "\n",
        "# ------------------ Data pipeline ------------------\n",
        "def preprocess_train(sample):\n",
        "#   Normalize images to [0,1].\n",
        "# One-hot encode labels for CategoricalCrossentropy\n",
        "    image = tf.cast(sample['image'], tf.float32) / 255.0\n",
        "    label = tf.one_hot(sample['label'], depth=10)\n",
        "\n",
        "    # Pad + crop + flip\n",
        "    image = tf.image.resize_with_crop_or_pad(image, 40, 40)\n",
        "    image = tf.image.random_crop(image, [32,32,3])\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "    # Simple extra augmentation\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def preprocess_test(sample):\n",
        "    image = tf.cast(sample['image'], tf.float32) / 255.0\n",
        "    label = tf.one_hot(sample['label'], depth=10)\n",
        "    return image, label\n",
        "\n",
        "def get_datasets(batch_size=512):\n",
        "    train_ds = tfds.load(\"cifar10\", split=\"train\", as_supervised=False)\n",
        "    test_ds  = tfds.load(\"cifar10\", split=\"test\",  as_supervised=False)\n",
        "\n",
        "    train_ds = (train_ds\n",
        "                .shuffle(50000)\n",
        "                .map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(batch_size)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "    test_ds = (test_ds\n",
        "                .map(preprocess_test, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(batch_size)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "    return train_ds, test_ds\n",
        "\n",
        "# ------------------ Cosine LR schedule with warmup ------------------\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "#   Warmup for first few epochs → LR linearly increases from 0 to base.\n",
        "\n",
        "# Then follows a cosine decay schedule → gradually lowers LR to near 0.\n",
        "\n",
        "# This is common in DAWNBench-style fast training (improves convergence).\n",
        "    def __init__(self, base_lr, steps, warmup_epochs=5):\n",
        "        super().__init__()\n",
        "        self.base_lr = base_lr\n",
        "        self.steps = steps\n",
        "        self.warmup_steps = warmup_epochs * steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        lr = 0.5 * self.base_lr * (1 + tf.cos(math.pi * step / self.steps))\n",
        "        if self.warmup_steps > 0:\n",
        "            warmup_lr = self.base_lr * step / tf.cast(self.warmup_steps, tf.float32)\n",
        "            lr = tf.where(step < self.warmup_steps, warmup_lr, lr)\n",
        "        return lr\n",
        "\n",
        "# ------------------ EMA callback ------------------\n",
        "# decay=0.999: means each update keeps 99.9% of old EMA weights, 0.1% of the new.\n",
        "\n",
        "# ema_weights: a list to store the smoothed weights.\n",
        "# At the very end of training:\n",
        "\n",
        "# Replace model’s actual weights with the EMA-smoothed weights.\n",
        "\n",
        "# So evaluation (validation/test) uses the stable version.\n",
        "class EMA(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, decay=0.999):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.ema_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.ema_weights = [w.numpy() for w in self.model.weights]\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        for i, w in enumerate(self.model.weights):\n",
        "            self.ema_weights[i] = self.decay * self.ema_weights[i] + (1. - self.decay) * w.numpy()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        # Assign EMA weights back for evaluation\n",
        "        for i, w in enumerate(self.model.weights):\n",
        "            w.assign(self.ema_weights[i])\n",
        "\n",
        "# ------------------ Training ------------------\n",
        "def train_cifar10(epochs=200, batch_size=512, base_lr=0.4):\n",
        "    train_ds, test_ds = get_datasets(batch_size)\n",
        "\n",
        "    model = ResNet18(num_classes=10)\n",
        "\n",
        "    steps_per_epoch = 50000 // batch_size\n",
        "    lr_schedule = WarmupCosine(base_lr, steps=steps_per_epoch*epochs)\n",
        "\n",
        "    optimizer = optimizers.AdamW(learning_rate=lr_schedule, weight_decay=5e-4)\n",
        "\n",
        "    loss_fn = losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss_fn,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    history = model.fit(train_ds,\n",
        "                        validation_data=test_ds,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=[EMA(decay=0.999)])\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_ds)\n",
        "    print(\"Final Test Accuracy (EMA weights):\", test_acc)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = train_cifar10(epochs=150, batch_size=512)\n",
        "    print(\"Best Val Accuracy:\", max(history.history['val_accuracy']))\n"
      ],
      "metadata": {
        "id": "xlLdzWsdlKp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5171e519-9718-4cfa-e0d1-cf369f1f4fa4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 347ms/step - accuracy: 0.2640 - loss: 2.1383 - val_accuracy: 0.1009 - val_loss: 2.6377\n",
            "Epoch 2/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 196ms/step - accuracy: 0.5445 - loss: 1.4908 - val_accuracy: 0.1000 - val_loss: 3.4299\n",
            "Epoch 3/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.6542 - loss: 1.2775 - val_accuracy: 0.2506 - val_loss: 3.0155\n",
            "Epoch 4/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.7305 - loss: 1.1287 - val_accuracy: 0.4599 - val_loss: 2.0079\n",
            "Epoch 5/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.7758 - loss: 1.0287 - val_accuracy: 0.3612 - val_loss: 2.3058\n",
            "Epoch 6/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.8096 - loss: 0.9552 - val_accuracy: 0.4530 - val_loss: 1.8292\n",
            "Epoch 7/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 199ms/step - accuracy: 0.8321 - loss: 0.9057 - val_accuracy: 0.5588 - val_loss: 1.5953\n",
            "Epoch 8/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.8403 - loss: 0.8856 - val_accuracy: 0.7223 - val_loss: 1.2258\n",
            "Epoch 9/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.8569 - loss: 0.8468 - val_accuracy: 0.6669 - val_loss: 1.2898\n",
            "Epoch 10/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 192ms/step - accuracy: 0.8712 - loss: 0.8131 - val_accuracy: 0.6578 - val_loss: 1.3728\n",
            "Epoch 11/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.8793 - loss: 0.7907 - val_accuracy: 0.6829 - val_loss: 1.3128\n",
            "Epoch 12/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.8865 - loss: 0.7689 - val_accuracy: 0.7196 - val_loss: 1.1751\n",
            "Epoch 13/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 198ms/step - accuracy: 0.8953 - loss: 0.7497 - val_accuracy: 0.7183 - val_loss: 1.1764\n",
            "Epoch 14/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9053 - loss: 0.7315 - val_accuracy: 0.7156 - val_loss: 1.2024\n",
            "Epoch 15/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9073 - loss: 0.7205 - val_accuracy: 0.7523 - val_loss: 1.1039\n",
            "Epoch 16/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 198ms/step - accuracy: 0.9107 - loss: 0.7145 - val_accuracy: 0.8119 - val_loss: 0.9691\n",
            "Epoch 17/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 196ms/step - accuracy: 0.9183 - loss: 0.6958 - val_accuracy: 0.8173 - val_loss: 0.9350\n",
            "Epoch 18/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9212 - loss: 0.6879 - val_accuracy: 0.8260 - val_loss: 0.9260\n",
            "Epoch 19/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 192ms/step - accuracy: 0.9261 - loss: 0.6784 - val_accuracy: 0.6949 - val_loss: 1.2994\n",
            "Epoch 20/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9285 - loss: 0.6685 - val_accuracy: 0.8187 - val_loss: 0.9524\n",
            "Epoch 21/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9319 - loss: 0.6633 - val_accuracy: 0.8067 - val_loss: 0.9685\n",
            "Epoch 22/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.9342 - loss: 0.6545 - val_accuracy: 0.7753 - val_loss: 1.0756\n",
            "Epoch 23/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9392 - loss: 0.6461 - val_accuracy: 0.7789 - val_loss: 1.0306\n",
            "Epoch 24/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9419 - loss: 0.6376 - val_accuracy: 0.7359 - val_loss: 1.1610\n",
            "Epoch 25/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9434 - loss: 0.6342 - val_accuracy: 0.7800 - val_loss: 1.0591\n",
            "Epoch 26/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9409 - loss: 0.6383 - val_accuracy: 0.8307 - val_loss: 0.8966\n",
            "Epoch 27/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9478 - loss: 0.6284 - val_accuracy: 0.8049 - val_loss: 0.9844\n",
            "Epoch 28/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9464 - loss: 0.6294 - val_accuracy: 0.8068 - val_loss: 0.9960\n",
            "Epoch 29/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9485 - loss: 0.6243 - val_accuracy: 0.6919 - val_loss: 1.3116\n",
            "Epoch 30/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.9512 - loss: 0.6171 - val_accuracy: 0.7445 - val_loss: 1.1929\n",
            "Epoch 31/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 196ms/step - accuracy: 0.9518 - loss: 0.6158 - val_accuracy: 0.8133 - val_loss: 0.9760\n",
            "Epoch 32/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 197ms/step - accuracy: 0.9542 - loss: 0.6101 - val_accuracy: 0.7452 - val_loss: 1.1642\n",
            "Epoch 33/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9566 - loss: 0.6009 - val_accuracy: 0.8034 - val_loss: 1.0058\n",
            "Epoch 34/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9581 - loss: 0.6010 - val_accuracy: 0.8434 - val_loss: 0.8957\n",
            "Epoch 35/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9587 - loss: 0.6003 - val_accuracy: 0.7913 - val_loss: 1.0488\n",
            "Epoch 36/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9582 - loss: 0.6005 - val_accuracy: 0.8305 - val_loss: 0.9270\n",
            "Epoch 37/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9603 - loss: 0.5977 - val_accuracy: 0.7418 - val_loss: 1.2233\n",
            "Epoch 38/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 198ms/step - accuracy: 0.9599 - loss: 0.5979 - val_accuracy: 0.8592 - val_loss: 0.8721\n",
            "Epoch 39/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9595 - loss: 0.5972 - val_accuracy: 0.8118 - val_loss: 0.9803\n",
            "Epoch 40/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9645 - loss: 0.5889 - val_accuracy: 0.7534 - val_loss: 1.1959\n",
            "Epoch 41/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9654 - loss: 0.5838 - val_accuracy: 0.7955 - val_loss: 1.0482\n",
            "Epoch 42/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9645 - loss: 0.5891 - val_accuracy: 0.8895 - val_loss: 0.7792\n",
            "Epoch 43/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9652 - loss: 0.5839 - val_accuracy: 0.8490 - val_loss: 0.8891\n",
            "Epoch 44/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9636 - loss: 0.5894 - val_accuracy: 0.8395 - val_loss: 0.9221\n",
            "Epoch 45/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9677 - loss: 0.5819 - val_accuracy: 0.8194 - val_loss: 0.9678\n",
            "Epoch 46/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9667 - loss: 0.5808 - val_accuracy: 0.8635 - val_loss: 0.8438\n",
            "Epoch 47/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 196ms/step - accuracy: 0.9674 - loss: 0.5802 - val_accuracy: 0.8494 - val_loss: 0.8882\n",
            "Epoch 48/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 196ms/step - accuracy: 0.9640 - loss: 0.5858 - val_accuracy: 0.8232 - val_loss: 0.9856\n",
            "Epoch 49/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9700 - loss: 0.5754 - val_accuracy: 0.8184 - val_loss: 0.9608\n",
            "Epoch 50/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9673 - loss: 0.5792 - val_accuracy: 0.8480 - val_loss: 0.8890\n",
            "Epoch 51/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9688 - loss: 0.5780 - val_accuracy: 0.8818 - val_loss: 0.8067\n",
            "Epoch 52/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9719 - loss: 0.5685 - val_accuracy: 0.8665 - val_loss: 0.8468\n",
            "Epoch 53/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9725 - loss: 0.5695 - val_accuracy: 0.8547 - val_loss: 0.8972\n",
            "Epoch 54/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9720 - loss: 0.5750 - val_accuracy: 0.8359 - val_loss: 0.9344\n",
            "Epoch 55/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9723 - loss: 0.5691 - val_accuracy: 0.8686 - val_loss: 0.8355\n",
            "Epoch 56/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9681 - loss: 0.5774 - val_accuracy: 0.8565 - val_loss: 0.8840\n",
            "Epoch 57/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 190ms/step - accuracy: 0.9717 - loss: 0.5699 - val_accuracy: 0.8719 - val_loss: 0.8394\n",
            "Epoch 58/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 192ms/step - accuracy: 0.9720 - loss: 0.5713 - val_accuracy: 0.8648 - val_loss: 0.8499\n",
            "Epoch 59/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9731 - loss: 0.5679 - val_accuracy: 0.7947 - val_loss: 1.0476\n",
            "Epoch 60/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9710 - loss: 0.5747 - val_accuracy: 0.8520 - val_loss: 0.8908\n",
            "Epoch 61/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 192ms/step - accuracy: 0.9736 - loss: 0.5678 - val_accuracy: 0.8258 - val_loss: 0.9573\n",
            "Epoch 62/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9723 - loss: 0.5678 - val_accuracy: 0.8533 - val_loss: 0.8907\n",
            "Epoch 63/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9736 - loss: 0.5668 - val_accuracy: 0.8655 - val_loss: 0.8418\n",
            "Epoch 64/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.9756 - loss: 0.5600 - val_accuracy: 0.8682 - val_loss: 0.8648\n",
            "Epoch 65/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9740 - loss: 0.5688 - val_accuracy: 0.7768 - val_loss: 1.1097\n",
            "Epoch 66/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9752 - loss: 0.5640 - val_accuracy: 0.8755 - val_loss: 0.8237\n",
            "Epoch 67/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9746 - loss: 0.5656 - val_accuracy: 0.8603 - val_loss: 0.8713\n",
            "Epoch 68/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9740 - loss: 0.5670 - val_accuracy: 0.8609 - val_loss: 0.8536\n",
            "Epoch 69/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9744 - loss: 0.5667 - val_accuracy: 0.8764 - val_loss: 0.8204\n",
            "Epoch 70/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9750 - loss: 0.5644 - val_accuracy: 0.8790 - val_loss: 0.8233\n",
            "Epoch 71/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9760 - loss: 0.5615 - val_accuracy: 0.8567 - val_loss: 0.8721\n",
            "Epoch 72/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9747 - loss: 0.5637 - val_accuracy: 0.8721 - val_loss: 0.8466\n",
            "Epoch 73/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9767 - loss: 0.5626 - val_accuracy: 0.8794 - val_loss: 0.8190\n",
            "Epoch 74/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 197ms/step - accuracy: 0.9776 - loss: 0.5587 - val_accuracy: 0.8104 - val_loss: 1.0042\n",
            "Epoch 75/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9748 - loss: 0.5652 - val_accuracy: 0.8877 - val_loss: 0.7872\n",
            "Epoch 76/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 195ms/step - accuracy: 0.9791 - loss: 0.5567 - val_accuracy: 0.8494 - val_loss: 0.9018\n",
            "Epoch 77/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9755 - loss: 0.5627 - val_accuracy: 0.8414 - val_loss: 0.9313\n",
            "Epoch 78/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9765 - loss: 0.5630 - val_accuracy: 0.8485 - val_loss: 0.9039\n",
            "Epoch 79/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9755 - loss: 0.5642 - val_accuracy: 0.8641 - val_loss: 0.8584\n",
            "Epoch 80/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 190ms/step - accuracy: 0.9757 - loss: 0.5628 - val_accuracy: 0.8630 - val_loss: 0.8554\n",
            "Epoch 81/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9785 - loss: 0.5559 - val_accuracy: 0.8872 - val_loss: 0.7960\n",
            "Epoch 82/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9806 - loss: 0.5526 - val_accuracy: 0.8851 - val_loss: 0.8029\n",
            "Epoch 83/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 198ms/step - accuracy: 0.9786 - loss: 0.5557 - val_accuracy: 0.8678 - val_loss: 0.8513\n",
            "Epoch 84/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 196ms/step - accuracy: 0.9783 - loss: 0.5582 - val_accuracy: 0.7717 - val_loss: 1.1314\n",
            "Epoch 85/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9765 - loss: 0.5606 - val_accuracy: 0.8664 - val_loss: 0.8471\n",
            "Epoch 86/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9773 - loss: 0.5566 - val_accuracy: 0.8818 - val_loss: 0.8177\n",
            "Epoch 87/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9765 - loss: 0.5617 - val_accuracy: 0.8724 - val_loss: 0.8369\n",
            "Epoch 88/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 195ms/step - accuracy: 0.9797 - loss: 0.5532 - val_accuracy: 0.8851 - val_loss: 0.7900\n",
            "Epoch 89/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.9792 - loss: 0.5535 - val_accuracy: 0.8699 - val_loss: 0.8298\n",
            "Epoch 90/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9793 - loss: 0.5535 - val_accuracy: 0.8695 - val_loss: 0.8496\n",
            "Epoch 91/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9782 - loss: 0.5556 - val_accuracy: 0.8691 - val_loss: 0.8318\n",
            "Epoch 92/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 198ms/step - accuracy: 0.9795 - loss: 0.5559 - val_accuracy: 0.8714 - val_loss: 0.8458\n",
            "Epoch 93/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 196ms/step - accuracy: 0.9819 - loss: 0.5483 - val_accuracy: 0.8921 - val_loss: 0.7800\n",
            "Epoch 94/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 195ms/step - accuracy: 0.9818 - loss: 0.5478 - val_accuracy: 0.8730 - val_loss: 0.8361\n",
            "Epoch 95/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9787 - loss: 0.5542 - val_accuracy: 0.8818 - val_loss: 0.8172\n",
            "Epoch 96/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9779 - loss: 0.5565 - val_accuracy: 0.8588 - val_loss: 0.8977\n",
            "Epoch 97/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9794 - loss: 0.5551 - val_accuracy: 0.8759 - val_loss: 0.8152\n",
            "Epoch 98/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9787 - loss: 0.5564 - val_accuracy: 0.8794 - val_loss: 0.8334\n",
            "Epoch 99/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9815 - loss: 0.5499 - val_accuracy: 0.8640 - val_loss: 0.8666\n",
            "Epoch 100/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9791 - loss: 0.5557 - val_accuracy: 0.8644 - val_loss: 0.8594\n",
            "Epoch 101/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 195ms/step - accuracy: 0.9802 - loss: 0.5508 - val_accuracy: 0.8643 - val_loss: 0.8423\n",
            "Epoch 102/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9817 - loss: 0.5494 - val_accuracy: 0.8603 - val_loss: 0.8775\n",
            "Epoch 103/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 197ms/step - accuracy: 0.9799 - loss: 0.5542 - val_accuracy: 0.8863 - val_loss: 0.8028\n",
            "Epoch 104/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9815 - loss: 0.5478 - val_accuracy: 0.8699 - val_loss: 0.8544\n",
            "Epoch 105/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 191ms/step - accuracy: 0.9798 - loss: 0.5541 - val_accuracy: 0.8597 - val_loss: 0.8889\n",
            "Epoch 106/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 192ms/step - accuracy: 0.9809 - loss: 0.5496 - val_accuracy: 0.8797 - val_loss: 0.8346\n",
            "Epoch 107/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9821 - loss: 0.5466 - val_accuracy: 0.8795 - val_loss: 0.8244\n",
            "Epoch 108/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 191ms/step - accuracy: 0.9829 - loss: 0.5463 - val_accuracy: 0.8900 - val_loss: 0.7968\n",
            "Epoch 109/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 192ms/step - accuracy: 0.9817 - loss: 0.5479 - val_accuracy: 0.8696 - val_loss: 0.8499\n",
            "Epoch 110/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9806 - loss: 0.5522 - val_accuracy: 0.8927 - val_loss: 0.7835\n",
            "Epoch 111/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9810 - loss: 0.5490 - val_accuracy: 0.8530 - val_loss: 0.8938\n",
            "Epoch 112/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9820 - loss: 0.5482 - val_accuracy: 0.8833 - val_loss: 0.8125\n",
            "Epoch 113/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9813 - loss: 0.5517 - val_accuracy: 0.8833 - val_loss: 0.8310\n",
            "Epoch 114/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9822 - loss: 0.5482 - val_accuracy: 0.8706 - val_loss: 0.8451\n",
            "Epoch 115/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9836 - loss: 0.5450 - val_accuracy: 0.8840 - val_loss: 0.8161\n",
            "Epoch 116/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9808 - loss: 0.5509 - val_accuracy: 0.8822 - val_loss: 0.8202\n",
            "Epoch 117/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.9857 - loss: 0.5420 - val_accuracy: 0.9005 - val_loss: 0.7563\n",
            "Epoch 118/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9823 - loss: 0.5460 - val_accuracy: 0.8622 - val_loss: 0.8762\n",
            "Epoch 119/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9834 - loss: 0.5438 - val_accuracy: 0.8682 - val_loss: 0.8886\n",
            "Epoch 120/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9839 - loss: 0.5435 - val_accuracy: 0.8851 - val_loss: 0.8085\n",
            "Epoch 121/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9835 - loss: 0.5455 - val_accuracy: 0.8600 - val_loss: 0.8809\n",
            "Epoch 122/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 192ms/step - accuracy: 0.9827 - loss: 0.5457 - val_accuracy: 0.8550 - val_loss: 0.9107\n",
            "Epoch 123/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 196ms/step - accuracy: 0.9849 - loss: 0.5415 - val_accuracy: 0.8807 - val_loss: 0.8309\n",
            "Epoch 124/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9828 - loss: 0.5461 - val_accuracy: 0.8738 - val_loss: 0.8487\n",
            "Epoch 125/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 192ms/step - accuracy: 0.9856 - loss: 0.5392 - val_accuracy: 0.8519 - val_loss: 0.9014\n",
            "Epoch 126/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.9846 - loss: 0.5420 - val_accuracy: 0.7765 - val_loss: 1.1083\n",
            "Epoch 127/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 195ms/step - accuracy: 0.9827 - loss: 0.5476 - val_accuracy: 0.8550 - val_loss: 0.8916\n",
            "Epoch 128/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9844 - loss: 0.5436 - val_accuracy: 0.8877 - val_loss: 0.8185\n",
            "Epoch 129/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9850 - loss: 0.5408 - val_accuracy: 0.8731 - val_loss: 0.8470\n",
            "Epoch 130/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 196ms/step - accuracy: 0.9843 - loss: 0.5414 - val_accuracy: 0.9077 - val_loss: 0.7535\n",
            "Epoch 131/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9842 - loss: 0.5415 - val_accuracy: 0.8873 - val_loss: 0.8233\n",
            "Epoch 132/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9864 - loss: 0.5404 - val_accuracy: 0.8827 - val_loss: 0.8170\n",
            "Epoch 133/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 192ms/step - accuracy: 0.9859 - loss: 0.5392 - val_accuracy: 0.8941 - val_loss: 0.7824\n",
            "Epoch 134/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 194ms/step - accuracy: 0.9840 - loss: 0.5416 - val_accuracy: 0.9094 - val_loss: 0.7391\n",
            "Epoch 135/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9871 - loss: 0.5347 - val_accuracy: 0.9008 - val_loss: 0.7672\n",
            "Epoch 136/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9871 - loss: 0.5363 - val_accuracy: 0.8914 - val_loss: 0.8025\n",
            "Epoch 137/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9849 - loss: 0.5436 - val_accuracy: 0.8798 - val_loss: 0.8258\n",
            "Epoch 138/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 189ms/step - accuracy: 0.9851 - loss: 0.5404 - val_accuracy: 0.8827 - val_loss: 0.8148\n",
            "Epoch 139/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9852 - loss: 0.5400 - val_accuracy: 0.8862 - val_loss: 0.8054\n",
            "Epoch 140/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9860 - loss: 0.5382 - val_accuracy: 0.8870 - val_loss: 0.7981\n",
            "Epoch 141/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9843 - loss: 0.5440 - val_accuracy: 0.8926 - val_loss: 0.7946\n",
            "Epoch 142/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 190ms/step - accuracy: 0.9863 - loss: 0.5391 - val_accuracy: 0.9063 - val_loss: 0.7644\n",
            "Epoch 143/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9869 - loss: 0.5366 - val_accuracy: 0.8854 - val_loss: 0.8157\n",
            "Epoch 144/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9881 - loss: 0.5341 - val_accuracy: 0.8678 - val_loss: 0.8655\n",
            "Epoch 145/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9868 - loss: 0.5354 - val_accuracy: 0.8923 - val_loss: 0.8015\n",
            "Epoch 146/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 190ms/step - accuracy: 0.9851 - loss: 0.5406 - val_accuracy: 0.8397 - val_loss: 0.9488\n",
            "Epoch 147/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.9840 - loss: 0.5409 - val_accuracy: 0.9041 - val_loss: 0.7574\n",
            "Epoch 148/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.9878 - loss: 0.5347 - val_accuracy: 0.8629 - val_loss: 0.8693\n",
            "Epoch 149/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 197ms/step - accuracy: 0.9871 - loss: 0.5357 - val_accuracy: 0.8568 - val_loss: 0.9103\n",
            "Epoch 150/150\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 195ms/step - accuracy: 0.9854 - loss: 0.5408 - val_accuracy: 0.8719 - val_loss: 0.8610\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9475 - loss: 0.6321\n",
            "Final Test Accuracy (EMA weights): 0.9455999732017517\n",
            "Best Val Accuracy: 0.9093999862670898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHPWOmgqxava"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}